{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f93872",
   "metadata": {},
   "source": [
    "# ðŸ”¥ PyTorch Fundamentals & MNIST Model Training\n",
    "\n",
    "## A Comprehensive Tutorial for Deep Learning\n",
    "\n",
    "---\n",
    "\n",
    "This notebook covers everything you need to know about PyTorch, from basic tensor operations to building and training neural networks on the MNIST dataset.\n",
    "\n",
    "### ðŸ“š Table of Contents\n",
    "\n",
    "1. **Import Required Libraries**\n",
    "2. **PyTorch Tensor Basics**\n",
    "3. **Mathematical Operations on Tensors**\n",
    "4. **Statistical Functions in PyTorch**\n",
    "5. **Automatic Differentiation with Autograd**\n",
    "6. **Gradient Calculation Examples**\n",
    "7. **Computation Graph Visualization**\n",
    "8. **Loading MNIST Dataset**\n",
    "9. **Implementing Custom DataLoader Class**\n",
    "10. **Building Neural Network Model Class**\n",
    "11. **Loss Functions in PyTorch**\n",
    "12. **Optimizers in PyTorch**\n",
    "13. **Training Loop Implementation**\n",
    "14. **Model Evaluation and Testing**\n",
    "15. **Deep Learning Modules Overview**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfac19a",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for our PyTorch journey. We'll use:\n",
    "- **torch**: The core PyTorch library\n",
    "- **torch.nn**: Neural network modules\n",
    "- **torch.optim**: Optimization algorithms\n",
    "- **torchvision**: Computer vision utilities and datasets\n",
    "- **matplotlib**: For visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2066d3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Torchvision for datasets and transforms\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# For computation graph visualization\n",
    "try:\n",
    "    from torchviz import make_dot\n",
    "    TORCHVIZ_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TORCHVIZ_AVAILABLE = False\n",
    "    print(\"Note: Install torchviz for computation graph visualization: pip install torchviz\")\n",
    "\n",
    "# Check PyTorch version and CUDA availability\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bf3718",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. PyTorch Tensor Basics\n",
    "\n",
    "### What are Tensors?\n",
    "\n",
    "**Tensors** are the fundamental data structure in PyTorch - they are multi-dimensional arrays similar to NumPy's ndarrays, but with additional capabilities:\n",
    "- Can run on GPUs for accelerated computing\n",
    "- Support automatic differentiation for deep learning\n",
    "- Optimized for neural network operations\n",
    "\n",
    "### Tensor Dimensions:\n",
    "- **0-D Tensor (Scalar)**: Single value\n",
    "- **1-D Tensor (Vector)**: Array of values\n",
    "- **2-D Tensor (Matrix)**: Table of values\n",
    "- **N-D Tensor**: Higher dimensional arrays (e.g., images, videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbbda83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TENSOR CREATION METHODS\n",
    "# ============================================\n",
    "\n",
    "# 1. Creating tensors from Python lists\n",
    "tensor_from_list = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(\"Tensor from list:\", tensor_from_list)\n",
    "\n",
    "# 2. Creating 2D tensor (matrix)\n",
    "matrix = torch.tensor([[1, 2, 3], \n",
    "                       [4, 5, 6]])\n",
    "print(f\"\\n2D Tensor (Matrix):\\n{matrix}\")\n",
    "\n",
    "# 3. Zeros tensor - useful for initialization\n",
    "zeros = torch.zeros(3, 4)  # 3 rows, 4 columns\n",
    "print(f\"\\nZeros tensor (3x4):\\n{zeros}\")\n",
    "\n",
    "# 4. Ones tensor\n",
    "ones = torch.ones(2, 3)\n",
    "print(f\"\\nOnes tensor (2x3):\\n{ones}\")\n",
    "\n",
    "# 5. Random tensor (uniform distribution between 0 and 1)\n",
    "rand_tensor = torch.rand(3, 3)\n",
    "print(f\"\\nRandom tensor (uniform):\\n{rand_tensor}\")\n",
    "\n",
    "# 6. Random tensor (normal distribution, mean=0, std=1)\n",
    "randn_tensor = torch.randn(3, 3)\n",
    "print(f\"\\nRandom tensor (normal):\\n{randn_tensor}\")\n",
    "\n",
    "# 7. Arange - similar to Python's range()\n",
    "arange_tensor = torch.arange(0, 10, 2)  # start, end, step\n",
    "print(f\"\\nArange tensor (0 to 10, step 2): {arange_tensor}\")\n",
    "\n",
    "# 8. Linspace - evenly spaced values\n",
    "linspace_tensor = torch.linspace(0, 1, 5)  # 5 values from 0 to 1\n",
    "print(f\"\\nLinspace tensor (0 to 1, 5 values): {linspace_tensor}\")\n",
    "\n",
    "# 9. Eye - Identity matrix\n",
    "identity = torch.eye(4)\n",
    "print(f\"\\nIdentity matrix (4x4):\\n{identity}\")\n",
    "\n",
    "# 10. Full - tensor filled with a specific value\n",
    "full_tensor = torch.full((2, 3), 7.0)\n",
    "print(f\"\\nFull tensor (filled with 7):\\n{full_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca75ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TENSOR ATTRIBUTES\n",
    "# ============================================\n",
    "\n",
    "sample_tensor = torch.rand(3, 4, 5)  # 3D tensor\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TENSOR ATTRIBUTES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Shape - dimensions of the tensor\n",
    "print(f\"Shape: {sample_tensor.shape}\")\n",
    "print(f\"Size (same as shape): {sample_tensor.size()}\")\n",
    "\n",
    "# Number of dimensions\n",
    "print(f\"Number of dimensions (ndim): {sample_tensor.ndim}\")\n",
    "\n",
    "# Data type\n",
    "print(f\"Data type (dtype): {sample_tensor.dtype}\")\n",
    "\n",
    "# Device (CPU or GPU)\n",
    "print(f\"Device: {sample_tensor.device}\")\n",
    "\n",
    "# Total number of elements\n",
    "print(f\"Total elements (numel): {sample_tensor.numel()}\")\n",
    "\n",
    "# Is it on CUDA?\n",
    "print(f\"Is CUDA tensor: {sample_tensor.is_cuda}\")\n",
    "\n",
    "# Requires gradient?\n",
    "print(f\"Requires gradient: {sample_tensor.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1186212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TENSOR DATA TYPES\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TENSOR DATA TYPES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Float tensors (default)\n",
    "float32_tensor = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(f\"Float32 (default): {float32_tensor.dtype}\")\n",
    "\n",
    "# Specifying data types\n",
    "float64_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float64)\n",
    "print(f\"Float64: {float64_tensor.dtype}\")\n",
    "\n",
    "int32_tensor = torch.tensor([1, 2, 3], dtype=torch.int32)\n",
    "print(f\"Int32: {int32_tensor.dtype}\")\n",
    "\n",
    "int64_tensor = torch.tensor([1, 2, 3], dtype=torch.int64)\n",
    "print(f\"Int64: {int64_tensor.dtype}\")\n",
    "\n",
    "bool_tensor = torch.tensor([True, False, True])\n",
    "print(f\"Boolean: {bool_tensor.dtype}\")\n",
    "\n",
    "# Type conversion using .to() or specific methods\n",
    "converted = float32_tensor.to(torch.int64)\n",
    "print(f\"\\nConverted float32 to int64: {converted}\")\n",
    "\n",
    "# Alternative conversion methods\n",
    "print(f\"Using .int(): {float32_tensor.int()}\")\n",
    "print(f\"Using .long(): {float32_tensor.long()}\")\n",
    "print(f\"Using .float(): {int32_tensor.float()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b83e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TENSOR RESHAPING OPERATIONS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TENSOR RESHAPING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "original = torch.arange(12)\n",
    "print(f\"Original tensor: {original}\")\n",
    "print(f\"Original shape: {original.shape}\")\n",
    "\n",
    "# Reshape to 3x4\n",
    "reshaped = original.reshape(3, 4)\n",
    "print(f\"\\nReshaped to (3, 4):\\n{reshaped}\")\n",
    "\n",
    "# View - similar to reshape but shares memory\n",
    "viewed = original.view(4, 3)\n",
    "print(f\"\\nView as (4, 3):\\n{viewed}\")\n",
    "\n",
    "# Flatten - convert to 1D\n",
    "flattened = reshaped.flatten()\n",
    "print(f\"\\nFlattened: {flattened}\")\n",
    "\n",
    "# Squeeze - remove dimensions of size 1\n",
    "tensor_with_ones = torch.rand(1, 3, 1, 4, 1)\n",
    "print(f\"\\nOriginal shape with 1s: {tensor_with_ones.shape}\")\n",
    "squeezed = tensor_with_ones.squeeze()\n",
    "print(f\"After squeeze: {squeezed.shape}\")\n",
    "\n",
    "# Unsqueeze - add a dimension of size 1\n",
    "tensor_2d = torch.rand(3, 4)\n",
    "print(f\"\\n2D tensor shape: {tensor_2d.shape}\")\n",
    "unsqueezed = tensor_2d.unsqueeze(0)  # Add dimension at position 0\n",
    "print(f\"After unsqueeze(0): {unsqueezed.shape}\")\n",
    "\n",
    "# Transpose\n",
    "matrix = torch.rand(2, 3)\n",
    "print(f\"\\nMatrix shape: {matrix.shape}\")\n",
    "transposed = matrix.T  # or matrix.transpose(0, 1)\n",
    "print(f\"Transposed shape: {transposed.shape}\")\n",
    "\n",
    "# Permute - reorder dimensions\n",
    "tensor_3d = torch.rand(2, 3, 4)\n",
    "print(f\"\\n3D tensor shape: {tensor_3d.shape}\")\n",
    "permuted = tensor_3d.permute(2, 0, 1)\n",
    "print(f\"Permuted (2,0,1) shape: {permuted.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab01fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TENSOR INDEXING AND SLICING\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TENSOR INDEXING AND SLICING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "tensor = torch.arange(20).reshape(4, 5)\n",
    "print(f\"Original tensor:\\n{tensor}\")\n",
    "\n",
    "# Single element access\n",
    "print(f\"\\nElement at [1, 2]: {tensor[1, 2]}\")\n",
    "\n",
    "# Row access\n",
    "print(f\"Row 0: {tensor[0]}\")\n",
    "print(f\"Row 1: {tensor[1, :]}\")\n",
    "\n",
    "# Column access\n",
    "print(f\"Column 2: {tensor[:, 2]}\")\n",
    "\n",
    "# Slicing\n",
    "print(f\"\\nRows 1-2, Columns 2-4:\\n{tensor[1:3, 2:5]}\")\n",
    "\n",
    "# Boolean indexing\n",
    "print(f\"\\nElements > 10: {tensor[tensor > 10]}\")\n",
    "\n",
    "# Fancy indexing\n",
    "indices = torch.tensor([0, 2, 3])\n",
    "print(f\"\\nRows at indices [0, 2, 3]:\\n{tensor[indices]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249fb1b4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Mathematical Operations on Tensors\n",
    "\n",
    "PyTorch provides a comprehensive set of mathematical operations that can be performed on tensors. These operations are optimized for both CPU and GPU computation.\n",
    "\n",
    "### Categories of Mathematical Operations:\n",
    "- **Element-wise operations**: Applied to each element independently\n",
    "- **Matrix operations**: Linear algebra operations\n",
    "- **Reduction operations**: Aggregate values across dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3ffc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ELEMENT-WISE OPERATIONS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ELEMENT-WISE OPERATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "a = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "b = torch.tensor([5.0, 6.0, 7.0, 8.0])\n",
    "\n",
    "print(f\"Tensor a: {a}\")\n",
    "print(f\"Tensor b: {b}\")\n",
    "\n",
    "# Addition\n",
    "print(f\"\\nAddition (a + b): {a + b}\")\n",
    "print(f\"torch.add(a, b): {torch.add(a, b)}\")\n",
    "\n",
    "# Subtraction\n",
    "print(f\"\\nSubtraction (a - b): {a - b}\")\n",
    "print(f\"torch.sub(a, b): {torch.sub(a, b)}\")\n",
    "\n",
    "# Multiplication (element-wise)\n",
    "print(f\"\\nMultiplication (a * b): {a * b}\")\n",
    "print(f\"torch.mul(a, b): {torch.mul(a, b)}\")\n",
    "\n",
    "# Division\n",
    "print(f\"\\nDivision (a / b): {a / b}\")\n",
    "print(f\"torch.div(a, b): {torch.div(a, b)}\")\n",
    "\n",
    "# Floor division\n",
    "print(f\"\\nFloor division (b // a): {b // a}\")\n",
    "\n",
    "# Modulo\n",
    "print(f\"\\nModulo (b % a): {b % a}\")\n",
    "\n",
    "# Power\n",
    "print(f\"\\nPower (a ** 2): {a ** 2}\")\n",
    "print(f\"torch.pow(a, 2): {torch.pow(a, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055689c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MATHEMATICAL FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MATHEMATICAL FUNCTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "print(f\"Tensor x: {x}\")\n",
    "\n",
    "# Exponential and logarithm\n",
    "print(f\"\\nexp(x): {torch.exp(x)}\")\n",
    "print(f\"log(x): {torch.log(x)}\")\n",
    "print(f\"log10(x): {torch.log10(x)}\")\n",
    "print(f\"log2(x): {torch.log2(x)}\")\n",
    "\n",
    "# Square root\n",
    "print(f\"\\nsqrt(x): {torch.sqrt(x)}\")\n",
    "\n",
    "# Absolute value\n",
    "y = torch.tensor([-1.0, -2.0, 3.0, 4.0])\n",
    "print(f\"\\nTensor y: {y}\")\n",
    "print(f\"abs(y): {torch.abs(y)}\")\n",
    "\n",
    "# Trigonometric functions\n",
    "angles = torch.tensor([0.0, np.pi/4, np.pi/2, np.pi])\n",
    "print(f\"\\nAngles (radians): {angles}\")\n",
    "print(f\"sin(angles): {torch.sin(angles)}\")\n",
    "print(f\"cos(angles): {torch.cos(angles)}\")\n",
    "print(f\"tan(angles[:3]): {torch.tan(angles[:3])}\")\n",
    "\n",
    "# Clamp - limit values to a range\n",
    "values = torch.tensor([-3, -1, 0, 2, 5, 10])\n",
    "print(f\"\\nOriginal values: {values}\")\n",
    "print(f\"Clamped (0, 5): {torch.clamp(values, min=0, max=5)}\")\n",
    "\n",
    "# Round, floor, ceil\n",
    "floats = torch.tensor([1.2, 2.5, 3.7, 4.1])\n",
    "print(f\"\\nFloat tensor: {floats}\")\n",
    "print(f\"Round: {torch.round(floats)}\")\n",
    "print(f\"Floor: {torch.floor(floats)}\")\n",
    "print(f\"Ceil: {torch.ceil(floats)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1517b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MATRIX OPERATIONS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MATRIX OPERATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "A = torch.tensor([[1., 2.], \n",
    "                  [3., 4.]])\n",
    "B = torch.tensor([[5., 6.], \n",
    "                  [7., 8.]])\n",
    "\n",
    "print(f\"Matrix A:\\n{A}\")\n",
    "print(f\"\\nMatrix B:\\n{B}\")\n",
    "\n",
    "# Matrix multiplication\n",
    "print(f\"\\nMatrix multiplication (A @ B):\\n{A @ B}\")\n",
    "print(f\"\\ntorch.matmul(A, B):\\n{torch.matmul(A, B)}\")\n",
    "print(f\"\\ntorch.mm(A, B):\\n{torch.mm(A, B)}\")\n",
    "\n",
    "# Dot product (for 1D tensors)\n",
    "v1 = torch.tensor([1., 2., 3.])\n",
    "v2 = torch.tensor([4., 5., 6.])\n",
    "print(f\"\\nVector v1: {v1}\")\n",
    "print(f\"Vector v2: {v2}\")\n",
    "print(f\"Dot product: {torch.dot(v1, v2)}\")\n",
    "\n",
    "# Matrix-vector multiplication\n",
    "v = torch.tensor([1., 2.])\n",
    "print(f\"\\nMatrix-vector (A @ v): {A @ v}\")\n",
    "\n",
    "# Batch matrix multiplication\n",
    "batch_A = torch.rand(3, 2, 4)  # 3 matrices of shape 2x4\n",
    "batch_B = torch.rand(3, 4, 3)  # 3 matrices of shape 4x3\n",
    "batch_result = torch.bmm(batch_A, batch_B)\n",
    "print(f\"\\nBatch multiplication: {batch_A.shape} @ {batch_B.shape} = {batch_result.shape}\")\n",
    "\n",
    "# Transpose\n",
    "print(f\"\\nTranspose of A:\\n{A.T}\")\n",
    "\n",
    "# Determinant\n",
    "print(f\"\\nDeterminant of A: {torch.det(A)}\")\n",
    "\n",
    "# Inverse\n",
    "print(f\"\\nInverse of A:\\n{torch.inverse(A)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82dbd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# BROADCASTING\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"BROADCASTING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Broadcasting allows operations on tensors of different shapes\n",
    "matrix = torch.ones(3, 3)\n",
    "vector = torch.tensor([1., 2., 3.])\n",
    "\n",
    "print(f\"Matrix (3x3):\\n{matrix}\")\n",
    "print(f\"\\nVector (3,): {vector}\")\n",
    "\n",
    "# Vector is broadcast across rows\n",
    "result = matrix + vector\n",
    "print(f\"\\nMatrix + Vector (broadcast):\\n{result}\")\n",
    "\n",
    "# Scalar broadcasting\n",
    "print(f\"\\nMatrix * 5:\\n{matrix * 5}\")\n",
    "\n",
    "# Column broadcasting (need to reshape)\n",
    "column = torch.tensor([[1.], [2.], [3.]])\n",
    "print(f\"\\nColumn vector:\\n{column}\")\n",
    "print(f\"\\nMatrix + Column (broadcast):\\n{matrix + column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd281b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IN-PLACE OPERATIONS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"IN-PLACE OPERATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# In-place operations modify the tensor directly and end with underscore (_)\n",
    "x = torch.tensor([1., 2., 3., 4.])\n",
    "print(f\"Original x: {x}\")\n",
    "\n",
    "# In-place addition\n",
    "x.add_(10)\n",
    "print(f\"After x.add_(10): {x}\")\n",
    "\n",
    "# In-place multiplication\n",
    "x.mul_(2)\n",
    "print(f\"After x.mul_(2): {x}\")\n",
    "\n",
    "# In-place zero\n",
    "x.zero_()\n",
    "print(f\"After x.zero_(): {x}\")\n",
    "\n",
    "# In-place fill\n",
    "x.fill_(7)\n",
    "print(f\"After x.fill_(7): {x}\")\n",
    "\n",
    "# âš ï¸ Warning: In-place operations can cause issues with autograd!\n",
    "print(\"\\nâš ï¸ Note: Be careful with in-place operations when using autograd!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad4ca07",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Statistical Functions in PyTorch\n",
    "\n",
    "Statistical functions are essential for data analysis and are frequently used in machine learning for:\n",
    "- Normalization\n",
    "- Batch statistics\n",
    "- Loss computation\n",
    "- Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31fee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STATISTICAL FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"STATISTICAL FUNCTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a sample tensor\n",
    "data = torch.tensor([[1., 2., 3., 4.],\n",
    "                     [5., 6., 7., 8.],\n",
    "                     [9., 10., 11., 12.]])\n",
    "\n",
    "print(f\"Data tensor:\\n{data}\")\n",
    "print(f\"Shape: {data.shape}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\n--- Basic Statistics ---\")\n",
    "print(f\"Mean: {torch.mean(data)}\")\n",
    "print(f\"Sum: {torch.sum(data)}\")\n",
    "print(f\"Standard Deviation: {torch.std(data)}\")\n",
    "print(f\"Variance: {torch.var(data)}\")\n",
    "\n",
    "# Min and Max\n",
    "print(f\"\\n--- Min/Max ---\")\n",
    "print(f\"Min: {torch.min(data)}\")\n",
    "print(f\"Max: {torch.max(data)}\")\n",
    "\n",
    "# Median\n",
    "print(f\"Median: {torch.median(data)}\")\n",
    "\n",
    "# Argmin and Argmax (index of min/max)\n",
    "print(f\"\\n--- Argmin/Argmax ---\")\n",
    "print(f\"Argmin (flattened): {torch.argmin(data)}\")\n",
    "print(f\"Argmax (flattened): {torch.argmax(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042a63c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DIMENSION-WISE OPERATIONS (using dim parameter)\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DIMENSION-WISE OPERATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "data = torch.tensor([[1., 2., 3., 4.],\n",
    "                     [5., 6., 7., 8.],\n",
    "                     [9., 10., 11., 12.]])\n",
    "\n",
    "print(f\"Data tensor:\\n{data}\")\n",
    "print(f\"Shape: {data.shape} (3 rows, 4 columns)\")\n",
    "\n",
    "# Sum along dimensions\n",
    "print(f\"\\n--- Sum along dimensions ---\")\n",
    "print(f\"Sum along dim=0 (columns): {torch.sum(data, dim=0)}\")  # Shape: (4,)\n",
    "print(f\"Sum along dim=1 (rows): {torch.sum(data, dim=1)}\")     # Shape: (3,)\n",
    "\n",
    "# Mean along dimensions\n",
    "print(f\"\\n--- Mean along dimensions ---\")\n",
    "print(f\"Mean along dim=0: {torch.mean(data, dim=0)}\")\n",
    "print(f\"Mean along dim=1: {torch.mean(data, dim=1)}\")\n",
    "\n",
    "# Keep dimensions (useful for broadcasting)\n",
    "print(f\"\\n--- Keeping dimensions ---\")\n",
    "mean_keepdim = torch.mean(data, dim=1, keepdim=True)\n",
    "print(f\"Mean with keepdim=True:\\n{mean_keepdim}\")\n",
    "print(f\"Shape: {mean_keepdim.shape}\")\n",
    "\n",
    "# Normalization example (subtract mean, divide by std)\n",
    "print(f\"\\n--- Normalization Example ---\")\n",
    "mean = torch.mean(data, dim=1, keepdim=True)\n",
    "std = torch.std(data, dim=1, keepdim=True)\n",
    "normalized = (data - mean) / std\n",
    "print(f\"Normalized data:\\n{normalized}\")\n",
    "\n",
    "# Max with indices\n",
    "print(f\"\\n--- Max with indices ---\")\n",
    "max_values, max_indices = torch.max(data, dim=1)\n",
    "print(f\"Max values per row: {max_values}\")\n",
    "print(f\"Max indices per row: {max_indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19180ff7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Automatic Differentiation with Autograd\n",
    "\n",
    "### What is Autograd?\n",
    "\n",
    "**Autograd** is PyTorch's automatic differentiation engine that powers neural network training. It automatically computes gradients (derivatives) of tensor operations.\n",
    "\n",
    "### Key Concepts:\n",
    "- **requires_grad=True**: Tells PyTorch to track operations on this tensor\n",
    "- **Computational Graph**: Records all operations for backpropagation\n",
    "- **backward()**: Computes gradients by traversing the graph backwards\n",
    "- **.grad**: Stores the computed gradient\n",
    "\n",
    "### Why is this important?\n",
    "In neural networks, we need to compute gradients of the loss function with respect to all parameters to update them during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eaa3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# AUTOGRAD BASICS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"AUTOGRAD BASICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create tensors with gradient tracking\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "print(f\"x = {x}\")\n",
    "print(f\"requires_grad: {x.requires_grad}\")\n",
    "\n",
    "# Perform operations - PyTorch builds a computation graph\n",
    "y = x ** 2\n",
    "print(f\"\\ny = xÂ² = {y}\")\n",
    "print(f\"y.grad_fn: {y.grad_fn}\")  # Shows the operation that created y\n",
    "\n",
    "z = y.sum()\n",
    "print(f\"\\nz = sum(y) = {z}\")\n",
    "print(f\"z.grad_fn: {z.grad_fn}\")\n",
    "\n",
    "# Compute gradients using backward()\n",
    "z.backward()\n",
    "\n",
    "# Access gradients\n",
    "print(f\"\\n--- After backward() ---\")\n",
    "print(f\"x.grad (dz/dx): {x.grad}\")\n",
    "# dz/dx = d(xâ‚Â² + xâ‚‚Â²)/dx = 2x\n",
    "# For x = [2, 3]: gradient = [4, 6] âœ“"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c71cac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Gradient Calculation Examples\n",
    "\n",
    "Let's explore more gradient computation examples and learn about important gradient-related operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae4f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# GRADIENT CALCULATION EXAMPLES\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"GRADIENT CALCULATION EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Example 1: Simple linear function\n",
    "# f(x) = 3x + 2, df/dx = 3\n",
    "print(\"--- Example 1: f(x) = 3x + 2 ---\")\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "f = 3 * x + 2\n",
    "f.backward()\n",
    "print(f\"x = {x.item()}\")\n",
    "print(f\"f(x) = {f.item()}\")\n",
    "print(f\"df/dx = {x.grad.item()}\")  # Should be 3\n",
    "\n",
    "# Example 2: Polynomial function\n",
    "# f(x) = xÂ³ + 2xÂ² + x, df/dx = 3xÂ² + 4x + 1\n",
    "print(\"\\n--- Example 2: f(x) = xÂ³ + 2xÂ² + x ---\")\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "f = x**3 + 2*x**2 + x\n",
    "f.backward()\n",
    "print(f\"x = {x.item()}\")\n",
    "print(f\"f(x) = {f.item()}\")\n",
    "print(f\"df/dx = {x.grad.item()}\")  # At x=2: 3(4) + 4(2) + 1 = 21\n",
    "\n",
    "# Example 3: Multiple variables\n",
    "print(\"\\n--- Example 3: f(x,y) = xÂ²y + yÂ³ ---\")\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "f = x**2 * y + y**3\n",
    "f.backward()\n",
    "print(f\"x = {x.item()}, y = {y.item()}\")\n",
    "print(f\"f(x,y) = {f.item()}\")\n",
    "print(f\"âˆ‚f/âˆ‚x = {x.grad.item()}\")  # 2xy = 2*2*3 = 12\n",
    "print(f\"âˆ‚f/âˆ‚y = {y.grad.item()}\")  # xÂ² + 3yÂ² = 4 + 27 = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f149cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# GRADIENT ACCUMULATION AND ZEROING\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"GRADIENT ACCUMULATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Gradients accumulate by default - very important to understand!\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# First backward\n",
    "y1 = x ** 2\n",
    "y1.backward()\n",
    "print(f\"After first backward: x.grad = {x.grad}\")\n",
    "\n",
    "# Second backward - gradients accumulate!\n",
    "y2 = x ** 2\n",
    "y2.backward()\n",
    "print(f\"After second backward: x.grad = {x.grad}\")  # Will be 12, not 6!\n",
    "\n",
    "# To prevent accumulation, zero the gradients\n",
    "x.grad.zero_()\n",
    "print(f\"After zeroing: x.grad = {x.grad}\")\n",
    "\n",
    "y3 = x ** 2\n",
    "y3.backward()\n",
    "print(f\"After third backward (with zeroing): x.grad = {x.grad}\")  # Now correct: 6\n",
    "\n",
    "print(\"\\nâš ï¸ Always zero gradients before backward() in training loops!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a965b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TORCH.NO_GRAD() CONTEXT MANAGER\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TORCH.NO_GRAD() CONTEXT MANAGER\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# With gradient tracking (default)\n",
    "y = x * 2\n",
    "print(f\"With gradients: y.requires_grad = {y.requires_grad}\")\n",
    "\n",
    "# Without gradient tracking - useful for inference\n",
    "with torch.no_grad():\n",
    "    z = x * 2\n",
    "    print(f\"With no_grad(): z.requires_grad = {z.requires_grad}\")\n",
    "\n",
    "# Operations inside no_grad() are not tracked\n",
    "print(f\"z has no grad_fn: {z.grad_fn}\")\n",
    "\n",
    "# Alternative: torch.inference_mode() (slightly faster)\n",
    "with torch.inference_mode():\n",
    "    w = x * 2\n",
    "    print(f\"With inference_mode(): w.requires_grad = {w.requires_grad}\")\n",
    "\n",
    "print(\"\\nâœ… Use torch.no_grad() during evaluation/inference for:\")\n",
    "print(\"   - Memory efficiency (no computation graph stored)\")\n",
    "print(\"   - Faster computation\")\n",
    "print(\"   - Preventing accidental gradient updates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad6f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DETACH AND REQUIRES_GRAD CONTROL\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DETACH AND REQUIRES_GRAD CONTROL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x * 2\n",
    "\n",
    "# Detach - creates a new tensor that doesn't require gradients\n",
    "z = y.detach()\n",
    "print(f\"y.requires_grad: {y.requires_grad}\")\n",
    "print(f\"z (detached).requires_grad: {z.requires_grad}\")\n",
    "\n",
    "# Detach shares memory with original!\n",
    "print(f\"\\nz shares memory with y: {z.data_ptr() == y.data_ptr()}\")\n",
    "\n",
    "# Using .data (less recommended, can cause issues)\n",
    "w = y.data\n",
    "print(f\"w (using .data).requires_grad: {w.requires_grad}\")\n",
    "\n",
    "# Changing requires_grad\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(f\"\\nOriginal requires_grad: {a.requires_grad}\")\n",
    "a.requires_grad_(True)  # In-place modification\n",
    "print(f\"After requires_grad_(True): {a.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051e0727",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Computation Graph Visualization\n",
    "\n",
    "The **computation graph** is a directed acyclic graph (DAG) that PyTorch builds as you perform operations. Each node represents an operation, and edges represent the data (tensors) flowing between operations.\n",
    "\n",
    "Understanding the computation graph is crucial for:\n",
    "- Debugging gradient issues\n",
    "- Understanding how backpropagation works\n",
    "- Optimizing model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7803ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPUTATION GRAPH VISUALIZATION\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"COMPUTATION GRAPH VISUALIZATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a simple computation graph\n",
    "x = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True)\n",
    "y = torch.tensor([[5., 6.], [7., 8.]], requires_grad=True)\n",
    "\n",
    "# Perform operations\n",
    "z = x * y           # Element-wise multiplication\n",
    "w = z.sum()         # Sum all elements\n",
    "v = w * 2           # Multiply by 2\n",
    "\n",
    "print(f\"x:\\n{x}\")\n",
    "print(f\"y:\\n{y}\")\n",
    "print(f\"z = x * y:\\n{z}\")\n",
    "print(f\"w = sum(z): {w}\")\n",
    "print(f\"v = w * 2: {v}\")\n",
    "\n",
    "# Visualize the computation graph\n",
    "if TORCHVIZ_AVAILABLE:\n",
    "    # Create visualization\n",
    "    dot = make_dot(v, params={'x': x, 'y': y})\n",
    "    dot.render('computation_graph', format='png', cleanup=True)\n",
    "    print(\"\\nâœ… Computation graph saved as 'computation_graph.png'\")\n",
    "    \n",
    "    # Display inline (if in Jupyter)\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(filename='computation_graph.png'))\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Install torchviz to visualize computation graphs:\")\n",
    "    print(\"   pip install torchviz graphviz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8418000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# UNDERSTANDING GRAD_FN\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"UNDERSTANDING GRAD_FN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# grad_fn shows the operation that created a tensor\n",
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "print(f\"x.grad_fn: {x.grad_fn}\")  # None - it's a leaf tensor\n",
    "\n",
    "y = x + 2\n",
    "print(f\"y = x + 2, y.grad_fn: {y.grad_fn}\")\n",
    "\n",
    "z = y * 3\n",
    "print(f\"z = y * 3, z.grad_fn: {z.grad_fn}\")\n",
    "\n",
    "w = z.mean()\n",
    "print(f\"w = mean(z), w.grad_fn: {w.grad_fn}\")\n",
    "\n",
    "# Tracing back through the graph\n",
    "print(\"\\n--- Tracing the graph ---\")\n",
    "current = w.grad_fn\n",
    "step = 0\n",
    "while current is not None:\n",
    "    print(f\"Step {step}: {current}\")\n",
    "    if hasattr(current, 'next_functions'):\n",
    "        parents = current.next_functions\n",
    "        if parents:\n",
    "            current = parents[0][0]  # Follow first parent\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dff6c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Loading MNIST Dataset\n",
    "\n",
    "### About MNIST\n",
    "**MNIST** (Modified National Institute of Standards and Technology) is a classic dataset containing:\n",
    "- **60,000 training images** of handwritten digits (0-9)\n",
    "- **10,000 test images**\n",
    "- Each image is **28Ã—28 grayscale**\n",
    "\n",
    "This is the \"Hello World\" of deep learning and perfect for learning neural networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda9b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LOADING MNIST DATASET\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"LOADING MNIST DATASET\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define transforms\n",
    "# 1. ToTensor(): Converts PIL Image to tensor and scales to [0, 1]\n",
    "# 2. Normalize(): Normalizes with mean and std (MNIST: 0.1307, 0.3081)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Download and load training dataset\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='./data',           # Directory to store data\n",
    "    train=True,              # Training set\n",
    "    download=True,           # Download if not present\n",
    "    transform=transform      # Apply transforms\n",
    ")\n",
    "\n",
    "# Download and load test dataset\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Examine a single sample\n",
    "sample_image, sample_label = train_dataset[0]\n",
    "print(f\"\\nSample image shape: {sample_image.shape}\")\n",
    "print(f\"Sample label: {sample_label}\")\n",
    "print(f\"Image dtype: {sample_image.dtype}\")\n",
    "print(f\"Image min: {sample_image.min():.4f}, max: {sample_image.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01c29b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZING MNIST SAMPLES\n",
    "# ============================================\n",
    "\n",
    "# Create a figure to display sample images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "fig.suptitle('MNIST Sample Images', fontsize=14)\n",
    "\n",
    "# Get 10 random samples\n",
    "indices = np.random.choice(len(train_dataset), 10, replace=False)\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    image, label = train_dataset[idx]\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    \n",
    "    # Convert tensor to numpy and remove channel dimension\n",
    "    img_np = image.squeeze().numpy()\n",
    "    \n",
    "    # Denormalize for better visualization\n",
    "    img_np = img_np * 0.3081 + 0.1307\n",
    "    \n",
    "    ax.imshow(img_np, cmap='gray')\n",
    "    ax.set_title(f'Label: {label}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display class distribution\n",
    "print(\"\\n--- Class Distribution ---\")\n",
    "labels = [train_dataset[i][1] for i in range(len(train_dataset))]\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "for digit, count in zip(unique, counts):\n",
    "    print(f\"Digit {digit}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c347f67f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Implementing Custom DataLoader Class\n",
    "\n",
    "### DataLoader Overview\n",
    "\n",
    "The **DataLoader** is a powerful utility that provides:\n",
    "- **Batching**: Groups samples into batches\n",
    "- **Shuffling**: Randomizes the order of data\n",
    "- **Parallel loading**: Uses multiple workers for efficiency\n",
    "- **Prefetching**: Loads next batches while training\n",
    "\n",
    "### Custom Dataset\n",
    "\n",
    "Sometimes you need to create your own Dataset class. This requires implementing:\n",
    "- `__init__()`: Initialize the dataset\n",
    "- `__len__()`: Return the size of the dataset  \n",
    "- `__getitem__()`: Return a sample given an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b24b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DATALOADER CLASS USAGE\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DATALOADER CLASS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,      # Number of samples per batch\n",
    "    shuffle=True,               # Shuffle data each epoch\n",
    "    num_workers=2,              # Parallel data loading processes\n",
    "    pin_memory=True             # Speed up data transfer to GPU\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,              # Don't shuffle test data\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")\n",
    "\n",
    "# Examine a batch\n",
    "batch_images, batch_labels = next(iter(train_loader))\n",
    "print(f\"\\nBatch images shape: {batch_images.shape}\")\n",
    "print(f\"Batch labels shape: {batch_labels.shape}\")\n",
    "print(f\"First 10 labels: {batch_labels[:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3720030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CUSTOM DATASET CLASS IMPLEMENTATION\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"CUSTOM DATASET CLASS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class CustomMNISTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for MNIST-like data.\n",
    "    \n",
    "    This demonstrates how to create your own Dataset class\n",
    "    for custom data sources.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            images: Tensor or array of images\n",
    "            labels: Tensor or array of labels\n",
    "            transform: Optional transforms to apply\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples.\"\"\"\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return a single sample.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the sample\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (image, label)\n",
    "        \"\"\"\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "# Create a small custom dataset for demonstration\n",
    "custom_images = torch.rand(100, 1, 28, 28)  # 100 random images\n",
    "custom_labels = torch.randint(0, 10, (100,))  # 100 random labels\n",
    "\n",
    "custom_dataset = CustomMNISTDataset(custom_images, custom_labels)\n",
    "\n",
    "print(f\"Custom dataset size: {len(custom_dataset)}\")\n",
    "sample_img, sample_lbl = custom_dataset[0]\n",
    "print(f\"Sample image shape: {sample_img.shape}\")\n",
    "print(f\"Sample label: {sample_lbl}\")\n",
    "\n",
    "# Create DataLoader from custom dataset\n",
    "custom_loader = DataLoader(custom_dataset, batch_size=16, shuffle=True)\n",
    "print(f\"Number of batches: {len(custom_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276d1f78",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Building Neural Network Model Class\n",
    "\n",
    "### nn.Module Overview\n",
    "\n",
    "In PyTorch, neural networks are built by subclassing `nn.Module`. Key components:\n",
    "\n",
    "- **`__init__()`**: Define all layers and learnable parameters\n",
    "- **`forward()`**: Define how data flows through the network\n",
    "\n",
    "### Key Layers We'll Use:\n",
    "- `nn.Linear`: Fully connected layer\n",
    "- `nn.Conv2d`: 2D convolutional layer\n",
    "- `nn.ReLU`: Activation function\n",
    "- `nn.Flatten`: Convert 2D to 1D\n",
    "- `nn.Dropout`: Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaab1a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SIMPLE FEED-FORWARD NEURAL NETWORK\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SIMPLE FEED-FORWARD NEURAL NETWORK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple feed-forward neural network for MNIST.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input: 784 (28x28 flattened)\n",
    "    - Hidden Layer 1: 512 neurons + ReLU + Dropout\n",
    "    - Hidden Layer 2: 256 neurons + ReLU + Dropout\n",
    "    - Output: 10 classes (digits 0-9)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        \n",
    "        # Flatten layer: (batch, 1, 28, 28) -> (batch, 784)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(784, 512)   # Input to hidden 1\n",
    "        self.fc2 = nn.Linear(512, 256)   # Hidden 1 to hidden 2\n",
    "        self.fc3 = nn.Linear(256, 10)    # Hidden 2 to output\n",
    "        \n",
    "        # Activation and regularization\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        # Flatten the input\n",
    "        x = self.flatten(x)              # (batch, 784)\n",
    "        \n",
    "        # First hidden layer\n",
    "        x = self.fc1(x)                  # (batch, 512)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Second hidden layer\n",
    "        x = self.fc2(x)                  # (batch, 256)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Output layer (no activation - CrossEntropyLoss handles it)\n",
    "        x = self.fc3(x)                  # (batch, 10)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model instance\n",
    "simple_model = SimpleNN()\n",
    "print(simple_model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in simple_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in simple_model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bec119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONVOLUTIONAL NEURAL NETWORK (CNN)\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"CONVOLUTIONAL NEURAL NETWORK (CNN)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A Convolutional Neural Network for MNIST.\n",
    "    \n",
    "    Architecture:\n",
    "    - Conv Layer 1: 1 -> 32 channels, 3x3 kernel\n",
    "    - Conv Layer 2: 32 -> 64 channels, 3x3 kernel\n",
    "    - MaxPool: 2x2\n",
    "    - Fully Connected: 9216 -> 128 -> 10\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,      # Grayscale input\n",
    "            out_channels=32,    # 32 feature maps\n",
    "            kernel_size=3,      # 3x3 filter\n",
    "            padding=1           # Same padding\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        # After 2 convs and 2 pools: 28 -> 14 -> 7, so 7x7x64 = 3136\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Conv block 1: Conv -> ReLU -> Pool\n",
    "        x = self.conv1(x)           # (batch, 32, 28, 28)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)            # (batch, 32, 14, 14)\n",
    "        \n",
    "        # Conv block 2: Conv -> ReLU -> Pool\n",
    "        x = self.conv2(x)           # (batch, 64, 14, 14)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)            # (batch, 64, 7, 7)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)   # (batch, 3136)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))     # (batch, 128)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)             # (batch, 10)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create CNN model\n",
    "cnn_model = CNN()\n",
    "print(cnn_model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in cnn_model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51855fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MODEL INSPECTION\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MODEL INSPECTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Access model parameters\n",
    "print(\"--- Named Parameters ---\")\n",
    "for name, param in cnn_model.named_parameters():\n",
    "    print(f\"{name}: shape={param.shape}, requires_grad={param.requires_grad}\")\n",
    "\n",
    "# Access specific layer\n",
    "print(\"\\n--- Accessing Layers ---\")\n",
    "print(f\"First conv layer: {cnn_model.conv1}\")\n",
    "print(f\"First conv layer weights shape: {cnn_model.conv1.weight.shape}\")\n",
    "print(f\"First conv layer bias shape: {cnn_model.conv1.bias.shape}\")\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n--- Device ---\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model to device\n",
    "cnn_model = cnn_model.to(device)\n",
    "print(f\"Model moved to: {next(cnn_model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8878ab1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Loss Functions in PyTorch\n",
    "\n",
    "### What is a Loss Function?\n",
    "\n",
    "A **loss function** (or cost function) measures how well our model's predictions match the true labels. The goal of training is to minimize this loss.\n",
    "\n",
    "### Common Loss Functions:\n",
    "\n",
    "| Loss Function | Use Case |\n",
    "|--------------|----------|\n",
    "| `nn.CrossEntropyLoss` | Multi-class classification |\n",
    "| `nn.NLLLoss` | Multi-class with log probabilities |\n",
    "| `nn.BCELoss` | Binary classification |\n",
    "| `nn.BCEWithLogitsLoss` | Binary (more numerically stable) |\n",
    "| `nn.MSELoss` | Regression |\n",
    "| `nn.L1Loss` | Regression (robust to outliers) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c808719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LOSS FUNCTIONS DEMONSTRATION\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"LOSS FUNCTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create sample predictions and targets\n",
    "# For multi-class classification (like MNIST)\n",
    "predictions = torch.randn(5, 10)  # 5 samples, 10 classes (raw logits)\n",
    "targets = torch.tensor([0, 3, 5, 7, 9])  # True class labels\n",
    "\n",
    "print(\"Predictions (logits) shape:\", predictions.shape)\n",
    "print(\"Targets:\", targets)\n",
    "\n",
    "# 1. CrossEntropyLoss - Most common for multi-class\n",
    "print(\"\\n--- CrossEntropyLoss ---\")\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "loss_value = ce_loss(predictions, targets)\n",
    "print(f\"CrossEntropyLoss: {loss_value.item():.4f}\")\n",
    "\n",
    "# Note: CrossEntropyLoss combines LogSoftmax + NLLLoss\n",
    "# So you should NOT apply softmax before CrossEntropyLoss!\n",
    "\n",
    "# 2. NLLLoss - Negative Log Likelihood (needs log probabilities)\n",
    "print(\"\\n--- NLLLoss ---\")\n",
    "nll_loss = nn.NLLLoss()\n",
    "log_probs = F.log_softmax(predictions, dim=1)  # Apply log_softmax first\n",
    "loss_value = nll_loss(log_probs, targets)\n",
    "print(f\"NLLLoss (with log_softmax): {loss_value.item():.4f}\")\n",
    "\n",
    "# 3. MSELoss - Mean Squared Error (for regression)\n",
    "print(\"\\n--- MSELoss ---\")\n",
    "mse_loss = nn.MSELoss()\n",
    "pred_reg = torch.randn(5)\n",
    "target_reg = torch.randn(5)\n",
    "loss_value = mse_loss(pred_reg, target_reg)\n",
    "print(f\"MSELoss: {loss_value.item():.4f}\")\n",
    "\n",
    "# 4. BCELoss - Binary Cross Entropy\n",
    "print(\"\\n--- BCELoss ---\")\n",
    "bce_loss = nn.BCELoss()\n",
    "pred_binary = torch.sigmoid(torch.randn(5))  # Must be probabilities (0-1)\n",
    "target_binary = torch.tensor([0., 1., 0., 1., 1.])\n",
    "loss_value = bce_loss(pred_binary, target_binary)\n",
    "print(f\"BCELoss: {loss_value.item():.4f}\")\n",
    "\n",
    "# 5. L1Loss - Mean Absolute Error\n",
    "print(\"\\n--- L1Loss ---\")\n",
    "l1_loss = nn.L1Loss()\n",
    "loss_value = l1_loss(pred_reg, target_reg)\n",
    "print(f\"L1Loss: {loss_value.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd15415",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Optimizers in PyTorch\n",
    "\n",
    "### What is an Optimizer?\n",
    "\n",
    "An **optimizer** updates the model's parameters based on the computed gradients to minimize the loss function.\n",
    "\n",
    "### Training Loop Pattern:\n",
    "```python\n",
    "optimizer.zero_grad()   # Clear previous gradients\n",
    "loss = criterion(output, target)  # Compute loss\n",
    "loss.backward()         # Compute gradients\n",
    "optimizer.step()        # Update parameters\n",
    "```\n",
    "\n",
    "### Popular Optimizers:\n",
    "\n",
    "| Optimizer | Description |\n",
    "|-----------|-------------|\n",
    "| `SGD` | Stochastic Gradient Descent |\n",
    "| `Adam` | Adaptive Moment Estimation |\n",
    "| `AdamW` | Adam with weight decay |\n",
    "| `RMSprop` | Root Mean Square Propagation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af7be38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# OPTIMIZERS DEMONSTRATION\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"OPTIMIZERS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a simple model for demonstration\n",
    "demo_model = nn.Linear(10, 2)\n",
    "\n",
    "# 1. SGD - Stochastic Gradient Descent\n",
    "print(\"--- SGD (Stochastic Gradient Descent) ---\")\n",
    "sgd_optimizer = optim.SGD(\n",
    "    demo_model.parameters(),\n",
    "    lr=0.01,           # Learning rate\n",
    "    momentum=0.9,      # Momentum factor\n",
    "    weight_decay=1e-4  # L2 regularization\n",
    ")\n",
    "print(f\"SGD: lr={sgd_optimizer.param_groups[0]['lr']}, momentum={sgd_optimizer.param_groups[0]['momentum']}\")\n",
    "\n",
    "# 2. Adam - Adaptive Moment Estimation\n",
    "print(\"\\n--- Adam ---\")\n",
    "adam_optimizer = optim.Adam(\n",
    "    demo_model.parameters(),\n",
    "    lr=0.001,          # Learning rate (default)\n",
    "    betas=(0.9, 0.999), # Coefficients for running averages\n",
    "    weight_decay=0     # L2 regularization\n",
    ")\n",
    "print(f\"Adam: lr={adam_optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "# 3. AdamW - Adam with decoupled weight decay\n",
    "print(\"\\n--- AdamW ---\")\n",
    "adamw_optimizer = optim.AdamW(\n",
    "    demo_model.parameters(),\n",
    "    lr=0.001,\n",
    "    weight_decay=0.01  # Better weight decay implementation\n",
    ")\n",
    "print(f\"AdamW: lr={adamw_optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "# 4. RMSprop\n",
    "print(\"\\n--- RMSprop ---\")\n",
    "rmsprop_optimizer = optim.RMSprop(\n",
    "    demo_model.parameters(),\n",
    "    lr=0.01,\n",
    "    alpha=0.99\n",
    ")\n",
    "print(f\"RMSprop: lr={rmsprop_optimizer.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297c9ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LEARNING RATE SCHEDULERS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"LEARNING RATE SCHEDULERS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Learning rate schedulers adjust LR during training\n",
    "demo_model = nn.Linear(10, 2)\n",
    "optimizer = optim.Adam(demo_model.parameters(), lr=0.1)\n",
    "\n",
    "# 1. StepLR - Decay by gamma every step_size epochs\n",
    "scheduler1 = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "print(f\"StepLR: Decay by 0.1 every 10 epochs\")\n",
    "\n",
    "# 2. ExponentialLR - Decay by gamma every epoch\n",
    "scheduler2 = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "print(f\"ExponentialLR: Decay by 0.95 every epoch\")\n",
    "\n",
    "# 3. ReduceLROnPlateau - Reduce when metric plateaus\n",
    "scheduler3 = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=5\n",
    ")\n",
    "print(f\"ReduceLROnPlateau: Reduce by 0.1 after 5 epochs of no improvement\")\n",
    "\n",
    "# 4. CosineAnnealingLR - Cosine annealing\n",
    "scheduler4 = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "print(f\"CosineAnnealingLR: Cosine schedule over 50 epochs\")\n",
    "\n",
    "# Demonstrate LR change\n",
    "print(\"\\n--- LR Schedule Demo (StepLR) ---\")\n",
    "for epoch in range(25):\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}: lr = {current_lr:.6f}\")\n",
    "    scheduler1.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81641280",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Training Loop Implementation\n",
    "\n",
    "Now let's put everything together and train our CNN on the MNIST dataset!\n",
    "\n",
    "### The Training Loop:\n",
    "1. Set model to training mode (`model.train()`)\n",
    "2. For each batch:\n",
    "   - Move data to device\n",
    "   - Zero gradients\n",
    "   - Forward pass\n",
    "   - Compute loss\n",
    "   - Backward pass\n",
    "   - Update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b7ba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SETUP FOR TRAINING\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TRAINING SETUP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "model = CNN().to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Training hyperparameters\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Create data loaders (recreate with appropriate settings)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"\\nHyperparameters:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "print(f\"  Optimizer: Adam\")\n",
    "print(f\"  Loss function: CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b005010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TRAINING FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network model\n",
    "        train_loader: DataLoader for training data\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        device: Device to use (CPU/GPU)\n",
    "        \n",
    "    Returns:\n",
    "        Average training loss for the epoch\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        # Move data to device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Print progress every 200 batches\n",
    "        if (batch_idx + 1) % 200 == 0:\n",
    "            print(f'  Batch [{batch_idx + 1}/{len(train_loader)}] '\n",
    "                  f'Loss: {loss.item():.4f} '\n",
    "                  f'Acc: {100. * correct / total:.2f}%')\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e831e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EVALUATION FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data.\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network model\n",
    "        test_loader: DataLoader for test data\n",
    "        criterion: Loss function\n",
    "        device: Device to use (CPU/GPU)\n",
    "        \n",
    "    Returns:\n",
    "        Average test loss and accuracy\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # No gradient computation during evaluation\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_acc = 100. * correct / total\n",
    "    \n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5468cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPLETE TRAINING LOOP\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TRAINING THE MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Track metrics\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch [{epoch + 1}/{NUM_EPOCHS}]\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\n  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}  | Test Acc: {test_acc:.2f}%\")\n",
    "    print(f\"  Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Final Test Accuracy: {test_accs[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc4db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PLOT TRAINING CURVES\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot Loss\n",
    "axes[0].plot(range(1, NUM_EPOCHS + 1), train_losses, 'b-o', label='Training Loss')\n",
    "axes[0].plot(range(1, NUM_EPOCHS + 1), test_losses, 'r-o', label='Test Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Test Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot Accuracy\n",
    "axes[1].plot(range(1, NUM_EPOCHS + 1), train_accs, 'b-o', label='Training Accuracy')\n",
    "axes[1].plot(range(1, NUM_EPOCHS + 1), test_accs, 'r-o', label='Test Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training and Test Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ed77a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14. Model Evaluation and Testing\n",
    "\n",
    "Now let's evaluate our trained model more thoroughly and visualize its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e107b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DETAILED EVALUATION\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"DETAILED EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get predictions on entire test set\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\n--- Per-Class Accuracy ---\")\n",
    "for digit in range(10):\n",
    "    mask = all_labels == digit\n",
    "    class_acc = (all_preds[mask] == all_labels[mask]).mean() * 100\n",
    "    print(f\"Digit {digit}: {class_acc:.2f}%\")\n",
    "\n",
    "# Overall accuracy\n",
    "overall_acc = (all_preds == all_labels).mean() * 100\n",
    "print(f\"\\nOverall Test Accuracy: {overall_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753af9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZE PREDICTIONS\n",
    "# ============================================\n",
    "\n",
    "# Get a batch of test images\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_images_device = test_images.to(device)\n",
    "    outputs = model(test_images_device)\n",
    "    probabilities = F.softmax(outputs, dim=1)\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "# Plot predictions\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "fig.suptitle('Model Predictions on Test Images', fontsize=14)\n",
    "\n",
    "for i in range(15):\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    \n",
    "    # Get image\n",
    "    img = test_images[i].squeeze().numpy()\n",
    "    img = img * 0.3081 + 0.1307  # Denormalize\n",
    "    \n",
    "    # Get prediction info\n",
    "    pred = predictions[i].item()\n",
    "    true = test_labels[i].item()\n",
    "    prob = probabilities[i][pred].item()\n",
    "    \n",
    "    # Display\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    color = 'green' if pred == true else 'red'\n",
    "    ax.set_title(f'Pred: {pred} ({prob:.1%})\\nTrue: {true}', color=color)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ac30af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFUSION MATRIX\n",
    "# ============================================\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Create confusion matrix manually\n",
    "confusion_matrix = np.zeros((10, 10), dtype=int)\n",
    "for true, pred in zip(all_labels, all_preds):\n",
    "    confusion_matrix[true, pred] += 1\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(confusion_matrix, cmap='Blues')\n",
    "\n",
    "# Add labels\n",
    "ax.set_xticks(range(10))\n",
    "ax.set_yticks(range(10))\n",
    "ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax.set_ylabel('True Label', fontsize=12)\n",
    "ax.set_title('Confusion Matrix', fontsize=14)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        text = ax.text(j, i, confusion_matrix[i, j],\n",
    "                      ha=\"center\", va=\"center\", \n",
    "                      color=\"white\" if confusion_matrix[i, j] > confusion_matrix.max()/2 else \"black\")\n",
    "\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find most common misclassifications\n",
    "print(\"\\n--- Most Common Misclassifications ---\")\n",
    "misclassified = [(true, pred, count) for (true, pred), count in \n",
    "                 Counter(zip(all_labels, all_preds)).items() if true != pred]\n",
    "misclassified.sort(key=lambda x: -x[2])\n",
    "\n",
    "for true, pred, count in misclassified[:5]:\n",
    "    print(f\"True: {true}, Predicted: {pred}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0bf8f6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 15. Deep Learning Modules Overview\n",
    "\n",
    "PyTorch provides a comprehensive set of modules for building neural networks. Let's explore the most important ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86ac77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# NN.SEQUENTIAL - BUILDING MODELS EASILY\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"NN.SEQUENTIAL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# nn.Sequential allows building models as a sequence of layers\n",
    "sequential_model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(784, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "print(\"Sequential Model:\")\n",
    "print(sequential_model)\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(1, 1, 28, 28)\n",
    "output = sequential_model(test_input)\n",
    "print(f\"\\nInput shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632ea97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ACTIVATION FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ACTIVATION FUNCTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "x = torch.linspace(-5, 5, 100)\n",
    "\n",
    "# Common activation functions\n",
    "activations = {\n",
    "    'ReLU': F.relu(x),\n",
    "    'LeakyReLU': F.leaky_relu(x, 0.1),\n",
    "    'Sigmoid': torch.sigmoid(x),\n",
    "    'Tanh': torch.tanh(x),\n",
    "    'GELU': F.gelu(x),\n",
    "    'SiLU (Swish)': F.silu(x)\n",
    "}\n",
    "\n",
    "# Plot activation functions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, y) in enumerate(activations.items()):\n",
    "    ax = axes[idx]\n",
    "    ax.plot(x.numpy(), y.numpy(), linewidth=2)\n",
    "    ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    ax.set_title(name, fontsize=12)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('f(x)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Common Activation Functions', y=1.02, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1edba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# NORMALIZATION LAYERS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"NORMALIZATION LAYERS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# BatchNorm - normalizes across batch dimension\n",
    "print(\"--- BatchNorm ---\")\n",
    "batch_norm_1d = nn.BatchNorm1d(100)  # For 1D data (N, C)\n",
    "batch_norm_2d = nn.BatchNorm2d(32)   # For 2D data (N, C, H, W)\n",
    "\n",
    "sample_1d = torch.randn(16, 100)  # Batch of 16, 100 features\n",
    "sample_2d = torch.randn(16, 32, 28, 28)  # Batch of 16, 32 channels\n",
    "\n",
    "output_1d = batch_norm_1d(sample_1d)\n",
    "output_2d = batch_norm_2d(sample_2d)\n",
    "\n",
    "print(f\"BatchNorm1d: input {sample_1d.shape} -> output {output_1d.shape}\")\n",
    "print(f\"BatchNorm2d: input {sample_2d.shape} -> output {output_2d.shape}\")\n",
    "\n",
    "# LayerNorm - normalizes across features\n",
    "print(\"\\n--- LayerNorm ---\")\n",
    "layer_norm = nn.LayerNorm([100])\n",
    "output_ln = layer_norm(sample_1d)\n",
    "print(f\"LayerNorm: input {sample_1d.shape} -> output {output_ln.shape}\")\n",
    "\n",
    "# Compare before and after normalization\n",
    "print(f\"\\nBefore BatchNorm - mean: {sample_1d.mean():.4f}, std: {sample_1d.std():.4f}\")\n",
    "print(f\"After BatchNorm - mean: {output_1d.mean():.4f}, std: {output_1d.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95f4fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# POOLING LAYERS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"POOLING LAYERS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create sample feature map\n",
    "feature_map = torch.randn(1, 1, 8, 8)\n",
    "print(f\"Input feature map shape: {feature_map.shape}\")\n",
    "\n",
    "# MaxPool2d - takes maximum value in each window\n",
    "max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "max_pooled = max_pool(feature_map)\n",
    "print(f\"\\nMaxPool2d (2x2): {feature_map.shape} -> {max_pooled.shape}\")\n",
    "\n",
    "# AvgPool2d - takes average value in each window\n",
    "avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "avg_pooled = avg_pool(feature_map)\n",
    "print(f\"AvgPool2d (2x2): {feature_map.shape} -> {avg_pooled.shape}\")\n",
    "\n",
    "# AdaptiveAvgPool2d - outputs fixed size\n",
    "adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling\n",
    "adaptive_pooled = adaptive_pool(feature_map)\n",
    "print(f\"AdaptiveAvgPool2d (1x1): {feature_map.shape} -> {adaptive_pooled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a64b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# OTHER IMPORTANT MODULES\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"OTHER IMPORTANT MODULES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Dropout - regularization\n",
    "print(\"--- Dropout ---\")\n",
    "dropout = nn.Dropout(p=0.5)  # 50% dropout probability\n",
    "x = torch.ones(1, 10)\n",
    "print(f\"Input: {x}\")\n",
    "# Note: Dropout behaves differently in train vs eval mode\n",
    "dropout.train()\n",
    "print(f\"Dropout (training): {dropout(x)}\")\n",
    "dropout.eval()\n",
    "print(f\"Dropout (eval): {dropout(x)}\")\n",
    "\n",
    "# Embedding - for categorical/text data\n",
    "print(\"\\n--- Embedding ---\")\n",
    "embedding = nn.Embedding(num_embeddings=1000, embedding_dim=128)  # 1000 words, 128-dim vectors\n",
    "word_indices = torch.tensor([1, 5, 10, 100])\n",
    "embedded = embedding(word_indices)\n",
    "print(f\"Word indices: {word_indices.shape} -> Embeddings: {embedded.shape}\")\n",
    "\n",
    "# Softmax\n",
    "print(\"\\n--- Softmax ---\")\n",
    "logits = torch.tensor([2.0, 1.0, 0.1])\n",
    "softmax = nn.Softmax(dim=0)\n",
    "probabilities = softmax(logits)\n",
    "print(f\"Logits: {logits}\")\n",
    "print(f\"Softmax: {probabilities}\")\n",
    "print(f\"Sum: {probabilities.sum():.4f}\")  # Should be 1.0\n",
    "\n",
    "# ModuleList - for dynamic architectures\n",
    "print(\"\\n--- ModuleList ---\")\n",
    "layers = nn.ModuleList([nn.Linear(10, 10) for _ in range(3)])\n",
    "print(f\"ModuleList with {len(layers)} layers\")\n",
    "for i, layer in enumerate(layers):\n",
    "    print(f\"  Layer {i}: {layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae41aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SAVING AND LOADING MODELS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SAVING AND LOADING MODELS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Method 1: Save entire model (not recommended)\n",
    "print(\"--- Method 1: Save entire model ---\")\n",
    "torch.save(model, 'model_complete.pth')\n",
    "print(\"Saved complete model to 'model_complete.pth'\")\n",
    "\n",
    "# Load entire model\n",
    "loaded_model = torch.load('model_complete.pth', weights_only=False)\n",
    "print(\"Loaded complete model\")\n",
    "\n",
    "# Method 2: Save state_dict only (recommended)\n",
    "print(\"\\n--- Method 2: Save state_dict (recommended) ---\")\n",
    "torch.save(model.state_dict(), 'model_state_dict.pth')\n",
    "print(\"Saved model state_dict to 'model_state_dict.pth'\")\n",
    "\n",
    "# Load state_dict\n",
    "new_model = CNN()  # Create new model instance\n",
    "new_model.load_state_dict(torch.load('model_state_dict.pth', weights_only=True))\n",
    "new_model.eval()  # Set to evaluation mode\n",
    "print(\"Loaded model state_dict\")\n",
    "\n",
    "# Method 3: Save checkpoint (for resuming training)\n",
    "print(\"\\n--- Method 3: Save checkpoint ---\")\n",
    "checkpoint = {\n",
    "    'epoch': NUM_EPOCHS,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'test_losses': test_losses,\n",
    "}\n",
    "torch.save(checkpoint, 'checkpoint.pth')\n",
    "print(\"Saved training checkpoint to 'checkpoint.pth'\")\n",
    "\n",
    "# Verify saved files\n",
    "import os\n",
    "for file in ['model_complete.pth', 'model_state_dict.pth', 'checkpoint.pth']:\n",
    "    size = os.path.getsize(file) / 1024\n",
    "    print(f\"{file}: {size:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1c13ac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ Summary\n",
    "\n",
    "### What We Covered:\n",
    "\n",
    "1. **Tensor Basics**: Creation, attributes, reshaping, indexing\n",
    "2. **Mathematical Operations**: Element-wise, matrix operations, broadcasting\n",
    "3. **Statistical Functions**: Mean, std, var, min, max with dimension control\n",
    "4. **Autograd**: Automatic differentiation, gradient computation\n",
    "5. **Computation Graphs**: How PyTorch tracks operations for backpropagation\n",
    "6. **MNIST Dataset**: Loading, transforming, and visualizing data\n",
    "7. **DataLoader**: Batching, shuffling, and custom datasets\n",
    "8. **Neural Network Models**: Building with nn.Module\n",
    "9. **Loss Functions**: CrossEntropyLoss, MSELoss, and others\n",
    "10. **Optimizers**: SGD, Adam, and learning rate schedulers\n",
    "11. **Training Loop**: Complete implementation with evaluation\n",
    "12. **Deep Learning Modules**: Normalization, pooling, activation functions\n",
    "13. **Saving/Loading**: Models and checkpoints\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- âœ… Use `requires_grad=True` for tensors that need gradients\n",
    "- âœ… Always call `optimizer.zero_grad()` before `loss.backward()`\n",
    "- âœ… Use `torch.no_grad()` during evaluation\n",
    "- âœ… Set `model.train()` for training, `model.eval()` for evaluation\n",
    "- âœ… Save `state_dict()` rather than the whole model\n",
    "- âœ… Use appropriate loss functions for your task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed25e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# QUICK REFERENCE - COMMON PYTORCH PATTERNS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"QUICK REFERENCE - COMMON PYTORCH PATTERNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ”· TENSOR CREATION:\n",
    "   torch.tensor([1, 2, 3])     # From list\n",
    "   torch.zeros(3, 4)           # All zeros\n",
    "   torch.ones(3, 4)            # All ones\n",
    "   torch.rand(3, 4)            # Uniform [0, 1)\n",
    "   torch.randn(3, 4)           # Normal (0, 1)\n",
    "   torch.arange(0, 10, 2)      # Range with step\n",
    "   torch.linspace(0, 1, 10)    # Evenly spaced\n",
    "\n",
    "ðŸ”· TENSOR OPERATIONS:\n",
    "   x + y, torch.add(x, y)      # Addition\n",
    "   x @ y, torch.matmul(x, y)   # Matrix multiplication\n",
    "   x.view(2, 3)                # Reshape (shares memory)\n",
    "   x.reshape(2, 3)             # Reshape (may copy)\n",
    "   x.T, x.transpose(0, 1)      # Transpose\n",
    "\n",
    "ðŸ”· AUTOGRAD:\n",
    "   x = torch.tensor([1.], requires_grad=True)\n",
    "   y = x ** 2\n",
    "   y.backward()                # Compute gradients\n",
    "   x.grad                      # Access gradient\n",
    "   \n",
    "ðŸ”· TRAINING LOOP:\n",
    "   model.train()               # Training mode\n",
    "   optimizer.zero_grad()       # Clear gradients\n",
    "   outputs = model(inputs)     # Forward pass\n",
    "   loss = criterion(outputs, targets)\n",
    "   loss.backward()             # Backward pass\n",
    "   optimizer.step()            # Update weights\n",
    "\n",
    "ðŸ”· EVALUATION:\n",
    "   model.eval()                # Evaluation mode\n",
    "   with torch.no_grad():       # Disable gradient tracking\n",
    "       outputs = model(inputs)\n",
    "\n",
    "ðŸ”· DEVICE MANAGEMENT:\n",
    "   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "   model = model.to(device)\n",
    "   data = data.to(device)\n",
    "\n",
    "ðŸ”· SAVING/LOADING:\n",
    "   torch.save(model.state_dict(), 'model.pth')\n",
    "   model.load_state_dict(torch.load('model.pth'))\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Congratulations! You've completed the PyTorch Fundamentals tutorial!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
